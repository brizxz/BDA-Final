#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
èšé¡çµæœåˆ†æå’Œæ¯”è¼ƒè…³æœ¬
"""

import pandas as pd
import numpy as np
import os

def analyze_clustering_results():
    """åˆ†ææ‰€æœ‰èšé¡çµæœ"""
    
    # ç®—æ³•åˆ—è¡¨
    algorithms = ['kmeans', 'dbscan', 'hdbscan', 'spectral', 'gmm', 'agglomerative']
    
    results_summary = []
    detailed_distributions = {}
    
    print("="*80)
    print("èšé¡ç®—æ³•çµæœåˆ†æ")
    print("="*80)
    
    for alg in algorithms:
        filename = f'{alg}_submission.csv'
        if os.path.exists(filename):
            print(f"\n=== {alg.upper()} ===")
            
            # è®€å–çµæœ
            df = pd.read_csv(filename)
            label_counts = df['label'].value_counts().sort_index()
            
            # åŸºæœ¬çµ±è¨ˆ
            n_clusters = len(label_counts)
            total_samples = len(df)
            max_cluster_size = label_counts.max()
            min_cluster_size = label_counts.min()
            
            # å¹³è¡¡æ€§æŒ‡æ¨™
            std_cluster_size = label_counts.std()
            imbalance_ratio = max_cluster_size / min_cluster_size if min_cluster_size > 0 else float('inf')
            
            # é¡¯ç¤ºè©³ç´°åˆ†ä½ˆ
            print(f"èšé¡æ•¸é‡: {n_clusters}")
            print(f"ç¸½æ¨£æœ¬æ•¸: {total_samples}")
            print(f"èšé¡åˆ†ä½ˆ:")
            for label, count in label_counts.items():
                percentage = count / total_samples * 100
                print(f"  èšé¡ {label}: {count:5d} æ¨£æœ¬ ({percentage:5.2f}%)")
            
            print(f"æœ€å¤§èšé¡å¤§å°: {max_cluster_size}")
            print(f"æœ€å°èšé¡å¤§å°: {min_cluster_size}")
            print(f"ä¸å¹³è¡¡æ¯”ä¾‹: {imbalance_ratio:.2f}")
            print(f"èšé¡å¤§å°æ¨™æº–å·®: {std_cluster_size:.2f}")
            
            # å„²å­˜åˆ°åŒ¯ç¸½
            results_summary.append({
                'ç®—æ³•': alg.upper(),
                'èšé¡æ•¸': n_clusters,
                'ç¸½æ¨£æœ¬': total_samples,
                'æœ€å¤§èšé¡': max_cluster_size,
                'æœ€å°èšé¡': min_cluster_size,
                'ä¸å¹³è¡¡æ¯”ä¾‹': f"{imbalance_ratio:.2f}" if imbalance_ratio != float('inf') else "âˆ",
                'æ¨™æº–å·®': f"{std_cluster_size:.1f}",
                'å¹³è¡¡åº¦': "å„ª" if imbalance_ratio < 2 else "è‰¯" if imbalance_ratio < 5 else "å·®"
            })
            
            detailed_distributions[alg] = label_counts
        else:
            print(f"æª”æ¡ˆä¸å­˜åœ¨: {filename}")
    
    # ç”Ÿæˆæ¯”è¼ƒè¡¨æ ¼
    print("\n" + "="*80)
    print("èšé¡ç®—æ³•æ¯”è¼ƒè¡¨æ ¼")
    print("="*80)
    
    if results_summary:
        summary_df = pd.DataFrame(results_summary)
        print(summary_df.to_string(index=False))
        
        # å„²å­˜è¡¨æ ¼åˆ°æª”æ¡ˆ
        summary_df.to_csv('clustering_comparison_table.csv', index=False, encoding='utf-8')
        print(f"\næ¯”è¼ƒè¡¨æ ¼å·²å„²å­˜è‡³: clustering_comparison_table.csv")
        
        # æª¢æŸ¥ç•°å¸¸æƒ…æ³
        print("\n" + "="*80)
        print("ç•°å¸¸æª¢æ¸¬")
        print("="*80)
        
        for _, row in summary_df.iterrows():
            alg = row['ç®—æ³•']
            balance_ratio = row['ä¸å¹³è¡¡æ¯”ä¾‹']
            
            if balance_ratio == "âˆ":
                print(f"âš ï¸  {alg}: å­˜åœ¨ç©ºèšé¡ï¼ˆæŸäº›èšé¡å¤§å°ç‚º0ï¼‰")
            elif balance_ratio != "âˆ" and float(balance_ratio) > 100:
                print(f"âš ï¸  {alg}: åš´é‡ä¸å¹³è¡¡ï¼ˆæ¯”ä¾‹ {balance_ratio}ï¼‰")
            elif balance_ratio != "âˆ" and float(balance_ratio) > 10:
                print(f"âš ï¸  {alg}: ä¸­åº¦ä¸å¹³è¡¡ï¼ˆæ¯”ä¾‹ {balance_ratio}ï¼‰")
        
        # ç‰¹åˆ¥åˆ†æagglomerativeçš„å•é¡Œ
        if 'agglomerative' in detailed_distributions:
            agg_dist = detailed_distributions['agglomerative']
            print(f"\nğŸ” å±¤æ¬¡èšé¡ç‰¹åˆ¥åˆ†æ:")
            print(f"   æ¨™ç±¤0: {agg_dist[0]} æ¨£æœ¬ ({agg_dist[0]/agg_dist.sum()*100:.4f}%)")
            print(f"   æ¨™ç±¤1: {agg_dist[1]} æ¨£æœ¬ ({agg_dist[1]/agg_dist.sum()*100:.4f}%)")
            print(f"   é€™è¡¨æ˜å±¤æ¬¡èšé¡å°‡å¹¾ä¹æ‰€æœ‰æ¨£æœ¬åˆ†é…åˆ°åŒä¸€å€‹èšé¡ä¸­")
            print(f"   å¯èƒ½çš„åŸå› ï¼š")
            print(f"   1. è³‡æ–™ç‰¹æ€§å°è‡´å±¤æ¬¡èšé¡å‚¾å‘æ–¼å½¢æˆä¸€å€‹å¤§èšé¡")
            print(f"   2. åƒæ•¸è¨­å®šä¸ç•¶")
            print(f"   3. è³‡æ–™é è™•ç†å•é¡Œ")
            print(f"   4. æ¼”ç®—æ³•åœ¨æ­¤è³‡æ–™é›†ä¸Šä¸é©ç”¨")
    
    return results_summary, detailed_distributions

def check_data_characteristics():
    """æª¢æŸ¥åŸå§‹æ•¸æ“šç‰¹æ€§"""
    print("\n" + "="*80)
    print("åŸå§‹æ•¸æ“šç‰¹æ€§åˆ†æ")
    print("="*80)
    
    # å˜—è©¦è¼‰å…¥åŸå§‹æ•¸æ“š
    data_files = ['data/public_data.csv', 'big data/public_data.csv', 'public_data.csv']
    data = None
    
    for file_path in data_files:
        if os.path.exists(file_path):
            data = pd.read_csv(file_path)
            print(f"æˆåŠŸè¼‰å…¥æ•¸æ“š: {file_path}")
            break
    
    if data is None:
        print("ç„¡æ³•æ‰¾åˆ°åŸå§‹æ•¸æ“šæª”æ¡ˆ")
        return
    
    print(f"æ•¸æ“šå½¢ç‹€: {data.shape}")
    print(f"ç‰¹å¾µæ¬„ä½: {list(data.columns)}")
    
    # æ’é™¤idæ¬„ä½
    feature_cols = [col for col in data.columns if col != 'id']
    features = data[feature_cols]
    
    print(f"\nç‰¹å¾µçµ±è¨ˆ:")
    print(features.describe())
    
    print(f"\nç‰¹å¾µç›¸é—œæ€§ï¼ˆçµ•å°å€¼å¹³å‡ï¼‰:")
    corr_matrix = features.corr()
    avg_corr = np.abs(corr_matrix).mean().mean()
    print(f"å¹³å‡ç›¸é—œæ€§: {avg_corr:.4f}")
    
    print(f"\nç‰¹å¾µç¯„åœ:")
    for col in feature_cols:
        min_val = features[col].min()
        max_val = features[col].max()
        range_val = max_val - min_val
        print(f"{col}: [{min_val:.4f}, {max_val:.4f}], ç¯„åœ={range_val:.4f}")

def main():
    """ä¸»å‡½æ•¸"""
    # æª¢æŸ¥ç•¶å‰ç›®éŒ„
    print(f"ç•¶å‰å·¥ä½œç›®éŒ„: {os.getcwd()}")
    
    # åˆ†æèšé¡çµæœ
    results_summary, detailed_distributions = analyze_clustering_results()
    
    # æª¢æŸ¥æ•¸æ“šç‰¹æ€§
    check_data_characteristics()
    
    print("\n" + "="*80)
    print("åˆ†æå®Œæˆï¼")
    print("="*80)

if __name__ == "__main__":
    main() 