#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
å¤§æ•¸æ“šåˆ†ææœŸæœ«å°ˆæ¡ˆ - Private Dataset é æ¸¬ (é«˜é€Ÿç‰ˆæœ¬)
ä½¿ç”¨GPUåŠ é€Ÿã€å¹³è¡Œè™•ç†å’Œç®—æ³•å„ªåŒ–ä¾†å¿«é€Ÿå®Œæˆé æ¸¬
æ”¯æ´æŒ‡å®šç‰¹å®šç®—æ³•é€²è¡Œé æ¸¬
"""

import pandas as pd
import numpy as np
import time
import warnings
import os
import argparse
from multiprocessing import Pool, cpu_count
from functools import partial
warnings.filterwarnings('ignore')

# æ¨™æº–æ©Ÿå™¨å­¸ç¿’åº«
from sklearn.cluster import KMeans, MiniBatchKMeans, DBSCAN, AgglomerativeClustering, SpectralClustering
from sklearn.mixture import GaussianMixture
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import silhouette_score
from sklearn.utils import resample

# é€²éšèšé¡ç®—æ³•
try:
    import hdbscan
    HDBSCAN_AVAILABLE = True
    print("âœ“ æª¢æ¸¬åˆ°HDBSCAN")
except ImportError:
    HDBSCAN_AVAILABLE = False
    print("âš ï¸  HDBSCANæœªå®‰è£ï¼Œå°‡è·³éHDBSCANç®—æ³•")

# GPUåŠ é€Ÿåº« (å¯é¸)
try:
    import cuml
    from cuml.cluster import KMeans as cuKMeans
    from cuml.preprocessing import StandardScaler as cuStandardScaler
    CUML_AVAILABLE = True
    print("âœ“ æª¢æ¸¬åˆ°GPUåŠ é€Ÿåº« cuML")
except ImportError:
    CUML_AVAILABLE = False
    print("âš ï¸  æœªå®‰è£cuMLï¼Œå°‡ä½¿ç”¨CPUç‰ˆæœ¬")

# è¨­ç½®ç’°å¢ƒè®Šæ•¸ä»¥æé«˜æ€§èƒ½
os.environ['OMP_NUM_THREADS'] = str(cpu_count())
os.environ['MKL_NUM_THREADS'] = str(cpu_count())

class FastPrivatePredictor:
    """é«˜é€Ÿç‰ˆæœ¬çš„Private Dataseté æ¸¬å™¨"""
    
    def __init__(self, use_gpu=True, use_sampling=True, n_jobs=-1, force_algorithm=None):
        self.public_data = None
        self.private_data = None
        self.features_public = None
        self.features_private = None
        self.feature_columns = None
        self.scaler = None
        self.best_model = None
        self.best_method = None
        self.best_params = None
        self.use_gpu = use_gpu and CUML_AVAILABLE
        self.use_sampling = use_sampling
        self.n_jobs = n_jobs if n_jobs != -1 else cpu_count()
        self.force_algorithm = force_algorithm
        
        print(f"ğŸš€ åˆå§‹åŒ–é«˜é€Ÿé æ¸¬å™¨")
        print(f"   GPUåŠ é€Ÿ: {'âœ“' if self.use_gpu else 'âœ—'}")
        print(f"   æ¡æ¨£å„ªåŒ–: {'âœ“' if self.use_sampling else 'âœ—'}")
        print(f"   CPUæ ¸å¿ƒæ•¸: {self.n_jobs}")
        if force_algorithm:
            print(f"   æŒ‡å®šç®—æ³•: {force_algorithm.upper()}")
        
    def load_data(self):
        """å¿«é€Ÿè¼‰å…¥æ•¸æ“š"""
        print("\nğŸ“‚ è¼‰å…¥æ•¸æ“š...")
        start_time = time.time()
        
        try:
            # ä½¿ç”¨æ›´å¿«çš„è®€å–æ–¹æ³•
            self.public_data = pd.read_csv("data/public_data.csv")
            self.private_data = pd.read_csv("data/private_data.csv")
            
            elapsed = time.time() - start_time
            print(f"âœ“ å…¬é–‹æ•¸æ“š: {self.public_data.shape}")
            print(f"âœ“ ç§æœ‰æ•¸æ“š: {self.private_data.shape}")
            print(f"âœ“ è¼‰å…¥å®Œæˆï¼Œè€—æ™‚: {elapsed:.1f}s")
            return True
            
        except Exception as e:
            print(f"âœ— è¼‰å…¥å¤±æ•—: {e}")
            return False
    
    def preprocess_data_fast(self):
        """é«˜é€Ÿæ•¸æ“šé è™•ç†"""
        print("\nâš¡ å¿«é€Ÿæ•¸æ“šé è™•ç†...")
        start_time = time.time()
        
        # æ‰¾åˆ°å…±åŒç‰¹å¾µåˆ—
        public_cols = set(self.public_data.columns)
        private_cols = set(self.private_data.columns)
        common_cols = public_cols.intersection(private_cols)
        self.feature_columns = [col for col in common_cols if col != 'id']
        
        print(f"âœ“ å…±åŒç‰¹å¾µ: {self.feature_columns}")
        
        # æå–ç‰¹å¾µæ•¸æ“š
        public_features_raw = self.public_data[self.feature_columns].values.astype(np.float32)
        private_features_raw = self.private_data[self.feature_columns].values.astype(np.float32)
        
        # é¸æ“‡ç¸®æ”¾å™¨
        if self.use_gpu:
            # ä½¿ç”¨GPUç¸®æ”¾å™¨
            self.scaler = cuStandardScaler()
            # è½‰æ›ç‚ºGPUæ ¼å¼
            import cudf
            public_df = cudf.DataFrame(public_features_raw)
            private_df = cudf.DataFrame(private_features_raw)
            
            self.features_public = self.scaler.fit_transform(public_df).values
            self.features_private = self.scaler.transform(private_df).values
        else:
            # ä½¿ç”¨CPUç¸®æ”¾å™¨ï¼Œä½†å„ªåŒ–è¨­ç½®
            self.scaler = StandardScaler()
            self.features_public = self.scaler.fit_transform(public_features_raw)
            self.features_private = self.scaler.transform(private_features_raw)
        
        elapsed = time.time() - start_time
        print(f"âœ“ å…¬é–‹æ•¸æ“šç‰¹å¾µ: {self.features_public.shape}")
        print(f"âœ“ ç§æœ‰æ•¸æ“šç‰¹å¾µ: {self.features_private.shape}")
        print(f"âœ“ é è™•ç†å®Œæˆï¼Œè€—æ™‚: {elapsed:.1f}s")
        
        return True
    
    def find_best_model_fast(self):
        """å¿«é€Ÿæ‰¾åˆ°æœ€ä½³æ¨¡å‹æˆ–ä½¿ç”¨æŒ‡å®šç®—æ³•"""
        print("\nğŸ” å¿«é€Ÿæ¨¡å‹é¸æ“‡...")
        
        # å¦‚æœæŒ‡å®šäº†ç‰¹å®šç®—æ³•ï¼Œåªä½¿ç”¨è©²ç®—æ³•
        if self.force_algorithm:
            return self._train_specific_algorithm(self.force_algorithm)
        
        # å¦‚æœä½¿ç”¨æ¡æ¨£ï¼Œå…ˆåœ¨å°æ¨£æœ¬ä¸Šå¿«é€Ÿæ¸¬è©¦
        if self.use_sampling and len(self.features_public) > 10000:
            print(f"ğŸ“Š ä½¿ç”¨æ¡æ¨£å„ªåŒ– (åŸæ•¸æ“š: {len(self.features_public)})")
            sample_size = min(10000, len(self.features_public) // 2)
            sample_indices = resample(range(len(self.features_public)), 
                                    n_samples=sample_size, 
                                    random_state=42)
            features_sample = self.features_public[sample_indices]
            print(f"ğŸ“Š æ¡æ¨£å¤§å°: {features_sample.shape}")
        else:
            features_sample = self.features_public
        
        # å¿«é€Ÿç®—æ³•æ¸¬è©¦
        algorithms = [
            ('fast_kmeans', partial(self._test_fast_kmeans, features_sample)),
            ('minibatch_kmeans', partial(self._test_minibatch_kmeans, features_sample)),
            ('fast_gmm', partial(self._test_fast_gmm, features_sample))
        ]
        
        if HDBSCAN_AVAILABLE:
            algorithms.append(('hdbscan', partial(self._test_hdbscan, features_sample)))
        
        if self.use_gpu:
            algorithms.insert(0, ('gpu_kmeans', partial(self._test_gpu_kmeans, features_sample)))
        
        best_score = -1
        best_result = None
        
        for method_name, test_func in algorithms:
            print(f"\nğŸ§ª æ¸¬è©¦ {method_name.upper()}...")
            start_time = time.time()
            
            try:
                result = test_func()
                elapsed = time.time() - start_time
                
                if result and result['score'] > best_score:
                    best_score = result['score']
                    best_result = result
                    self.best_method = method_name
                    # ç”¨å®Œæ•´æ•¸æ“šé‡æ–°è¨“ç·´æœ€ä½³æ¨¡å‹
                    if self.use_sampling and len(self.features_public) > 10000:
                        best_result = self._retrain_best_model(result)
                    self.best_model = best_result['model']
                    self.best_params = best_result['params']
                    
                    print(f"âœ“ {method_name}: Score={result['score']:.4f}, "
                          f"èšé¡æ•¸={result['n_clusters']}, è€—æ™‚={elapsed:.1f}s")
                else:
                    print(f"âœ— {method_name}: è¡¨ç¾ä¸ä½³æˆ–å¤±æ•—")
                    
            except Exception as e:
                print(f"âœ— {method_name}: å¤±æ•— ({str(e)[:50]})")
        
        if best_result:
            print(f"\nğŸ† æœ€ä½³æ¨¡å‹: {self.best_method.upper()}")
            print(f"   Silhouette Score: {best_score:.4f}")
            print(f"   èšé¡æ•¸: {best_result['n_clusters']}")
            return True
        else:
            print("\nâœ— ç„¡æ³•æ‰¾åˆ°æœ‰æ•ˆæ¨¡å‹")
            return False
    
    def _train_specific_algorithm(self, algorithm):
        """è¨“ç·´æŒ‡å®šçš„ç‰¹å®šç®—æ³•"""
        print(f"ğŸ¯ ä½¿ç”¨æŒ‡å®šç®—æ³•: {algorithm.upper()}")
        
        # æ ¹æ“šæŒ‡å®šç®—æ³•é¸æ“‡å°æ‡‰çš„æ¸¬è©¦å‡½æ•¸
        test_functions = {
            'kmeans': self._test_fast_kmeans,
            'fast_kmeans': self._test_fast_kmeans,
            'minibatch_kmeans': self._test_minibatch_kmeans,
            'gmm': self._test_fast_gmm,
            'fast_gmm': self._test_fast_gmm,
            'hdbscan': self._test_hdbscan,
            'dbscan': self._test_dbscan,
            'agglomerative': self._test_agglomerative,
            'spectral': self._test_spectral,
        }
        
        if self.use_gpu and algorithm in ['gpu_kmeans']:
            test_func = self._test_gpu_kmeans
        elif algorithm in test_functions:
            test_func = test_functions[algorithm]
        else:
            print(f"âœ— ä¸æ”¯æ´çš„ç®—æ³•: {algorithm}")
            return False
        
        # é¸æ“‡è¨“ç·´æ•¸æ“š
        if self.use_sampling and len(self.features_public) > 10000 and algorithm != 'hdbscan':
            sample_size = min(10000, len(self.features_public) // 2)
            sample_indices = resample(range(len(self.features_public)), 
                                    n_samples=sample_size, 
                                    random_state=42)
            features_sample = self.features_public[sample_indices]
            print(f"ğŸ“Š æ¡æ¨£å¤§å°: {features_sample.shape}")
        else:
            features_sample = self.features_public
            print(f"ğŸ“Š ä½¿ç”¨å®Œæ•´æ•¸æ“š: {features_sample.shape}")
        
        start_time = time.time()
        result = test_func(features_sample)
        elapsed = time.time() - start_time
        
        if result:
            # å¦‚æœä½¿ç”¨äº†æ¡æ¨£ï¼Œåœ¨å®Œæ•´æ•¸æ“šä¸Šé‡æ–°è¨“ç·´
            if self.use_sampling and len(self.features_public) > 10000 and algorithm != 'hdbscan':
                self.best_method = algorithm
                result = self._retrain_best_model(result)
            
            self.best_model = result['model']
            self.best_method = algorithm
            self.best_params = result['params']
            
            print(f"âœ“ {algorithm.upper()}: Score={result['score']:.4f}, "
                  f"èšé¡æ•¸={result['n_clusters']}, è€—æ™‚={elapsed:.1f}s")
            return True
        else:
            print(f"âœ— {algorithm.upper()}: è¨“ç·´å¤±æ•—")
            return False
    
    def _test_hdbscan(self, features):
        """æ¸¬è©¦HDBSCANç®—æ³•"""
        if not HDBSCAN_AVAILABLE:
            print("âœ— HDBSCANä¸å¯ç”¨")
            return None
            
        min_cluster_sizes = [5, 10, 15, 20, 30, 50]
        best_size = 10
        best_score = -1
        best_model = None
        
        for min_cluster_size in min_cluster_sizes:
            try:
                hdbscan_model = hdbscan.HDBSCAN(
                    min_cluster_size=min_cluster_size,
                    min_samples=5,
                    cluster_selection_epsilon=0.0
                )
                labels = hdbscan_model.fit_predict(features)
                
                # éæ¿¾å™ªè²é»
                valid_mask = labels != -1
                if sum(valid_mask) < len(features) * 0.1:  # å¦‚æœæœ‰æ•ˆé»å¤ªå°‘ï¼Œè·³é
                    continue
                    
                valid_labels = labels[valid_mask]
                valid_features = features[valid_mask]
                
                if len(set(valid_labels)) >= 2:
                    score = silhouette_score(valid_features, valid_labels)
                    
                    if score > best_score:
                        best_score = score
                        best_size = min_cluster_size
                        best_model = hdbscan_model
                        
                    print(f"  HDBSCAN min_cluster_size={min_cluster_size}: "
                          f"Score={score:.4f}, èšé¡æ•¸={len(set(valid_labels))}, "
                          f"å™ªè²é»={sum(labels == -1)}")
                
            except Exception as e:
                print(f"  HDBSCAN min_cluster_size={min_cluster_size}: å¤±æ•—")
                continue
        
        if best_model:
            return {
                'model': best_model,
                'score': best_score,
                'n_clusters': len(set(best_model.labels_[best_model.labels_ != -1])),
                'params': {'min_cluster_size': best_size}
            }
        return None
    
    def _test_dbscan(self, features):
        """æ¸¬è©¦DBSCANç®—æ³•"""
        eps_values = [0.3, 0.5, 0.7, 1.0, 1.5]
        min_samples_values = [5, 10, 15]
        
        best_eps = 0.5
        best_min_samples = 5
        best_score = -1
        best_model = None
        
        for eps in eps_values:
            for min_samples in min_samples_values:
                try:
                    dbscan = DBSCAN(eps=eps, min_samples=min_samples)
                    labels = dbscan.fit_predict(features)
                    
                    valid_mask = labels != -1
                    if sum(valid_mask) < len(features) * 0.1:
                        continue
                        
                    valid_labels = labels[valid_mask]
                    valid_features = features[valid_mask]
                    
                    if len(set(valid_labels)) >= 2:
                        score = silhouette_score(valid_features, valid_labels)
                        
                        if score > best_score:
                            best_score = score
                            best_eps = eps
                            best_min_samples = min_samples
                            best_model = dbscan
                            
                        print(f"  DBSCAN eps={eps}, min_samples={min_samples}: "
                              f"Score={score:.4f}")
                        
                except Exception:
                    continue
        
        if best_model:
            return {
                'model': best_model,
                'score': best_score,
                'n_clusters': len(set(best_model.labels_[best_model.labels_ != -1])),
                'params': {'eps': best_eps, 'min_samples': best_min_samples}
            }
        return None
    
    def _test_agglomerative(self, features):
        """æ¸¬è©¦å±¤æ¬¡èšé¡ç®—æ³•"""
        k_values = [2, 3, 4, 5, 6, 8, 10]
        best_k = 2
        best_score = -1
        best_model = None
        
        for k in k_values:
            try:
                agg = AgglomerativeClustering(n_clusters=k)
                labels = agg.fit_predict(features)
                
                score = silhouette_score(features, labels)
                
                if score > best_score:
                    best_score = score
                    best_k = k
                    best_model = agg
                    
                print(f"  å±¤æ¬¡èšé¡ K={k}: Score={score:.4f}")
                
            except Exception:
                continue
        
        if best_model:
            return {
                'model': best_model,
                'score': best_score,
                'n_clusters': best_k,
                'params': {'n_clusters': best_k}
            }
        return None
    
    def _test_spectral(self, features):
        """æ¸¬è©¦è­œèšé¡ç®—æ³•"""
        k_values = [2, 3, 4, 5, 6, 8]
        best_k = 2
        best_score = -1
        best_model = None
        
        for k in k_values:
            try:
                spectral = SpectralClustering(n_clusters=k, random_state=42)
                labels = spectral.fit_predict(features)
                
                score = silhouette_score(features, labels)
                
                if score > best_score:
                    best_score = score
                    best_k = k
                    best_model = spectral
                    
                print(f"  è­œèšé¡ K={k}: Score={score:.4f}")
                
            except Exception:
                continue
        
        if best_model:
            return {
                'model': best_model,
                'score': best_score,
                'n_clusters': best_k,
                'params': {'n_clusters': best_k}
            }
        return None

    def _test_gpu_kmeans(self, features):
        """æ¸¬è©¦GPUåŠ é€Ÿçš„K-Means"""
        if not self.use_gpu:
            return None
            
        k_values = [2, 3, 4, 5, 6]  # åªæ¸¬è©¦å°‘æ•¸Kå€¼
        best_k = 2
        best_score = -1
        best_model = None
        
        for k in k_values:
            try:
                kmeans = cuKMeans(n_clusters=k, random_state=42, max_iter=100)
                labels = kmeans.fit_predict(features)
                
                # è½‰æ›å›CPUè¨ˆç®—score
                labels_cpu = labels.to_pandas().values if hasattr(labels, 'to_pandas') else labels
                features_cpu = features.to_pandas().values if hasattr(features, 'to_pandas') else features
                
                score = silhouette_score(features_cpu, labels_cpu)
                
                if score > best_score:
                    best_score = score
                    best_k = k
                    best_model = kmeans
                    
                print(f"  GPU K={k}: Score={score:.4f}")
                
            except Exception as e:
                print(f"  GPU K={k}: å¤±æ•—")
                continue
        
        if best_model:
            return {
                'model': best_model,
                'score': best_score,
                'n_clusters': best_k,
                'params': {'n_clusters': best_k}
            }
        return None
    
    def _test_fast_kmeans(self, features):
        """æ¸¬è©¦å¿«é€ŸK-Means"""
        k_values = [2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30]
        best_k = 2
        best_score = -1
        best_model = None
        
        for k in k_values:
            try:
                # ä½¿ç”¨è¼ƒå°‘çš„åˆå§‹åŒ–æ¬¡æ•¸å’Œè¿­ä»£æ¬¡æ•¸
                kmeans = KMeans(n_clusters=k, random_state=42, 
                              n_init=3, max_iter=50, algorithm='elkan')
                labels = kmeans.fit_predict(features)
                
                score = silhouette_score(features, labels)
                
                if score > best_score:
                    best_score = score
                    best_k = k
                    best_model = kmeans
                    
                print(f"  å¿«é€ŸK={k}: Score={score:.4f}")
                
            except Exception:
                continue
        
        if best_model:
            return {
                'model': best_model,
                'score': best_score,
                'n_clusters': best_k,
                'params': {'n_clusters': best_k}
            }
        return None
    
    def _test_minibatch_kmeans(self, features):
        """æ¸¬è©¦MiniBatch K-Means (é©åˆå¤§æ•¸æ“š)"""
        k_values = [2, 3, 4, 5, 6]
        best_k = 2
        best_score = -1
        best_model = None
        
        for k in k_values:
            try:
                # MiniBatchç‰ˆæœ¬ï¼Œé€Ÿåº¦æ›´å¿«
                kmeans = MiniBatchKMeans(n_clusters=k, random_state=42,
                                       batch_size=1000, max_iter=100)
                labels = kmeans.fit_predict(features)
                
                score = silhouette_score(features, labels)
                
                if score > best_score:
                    best_score = score
                    best_k = k
                    best_model = kmeans
                    
                print(f"  MiniBatch K={k}: Score={score:.4f}")
                
            except Exception:
                continue
        
        if best_model:
            return {
                'model': best_model,
                'score': best_score,
                'n_clusters': best_k,
                'params': {'n_clusters': best_k}
            }
        return None
    
    def _test_fast_gmm(self, features):
        """æ¸¬è©¦å¿«é€ŸGMM"""
        k_values = [2, 3, 4, 5]  # æ¸›å°‘æ¸¬è©¦æ•¸é‡
        best_k = 2
        best_score = -1
        best_model = None
        
        for k in k_values:
            try:
                # æ¸›å°‘è¿­ä»£æ¬¡æ•¸
                gmm = GaussianMixture(n_components=k, random_state=42, 
                                    max_iter=50, tol=1e-3)
                labels = gmm.fit_predict(features)
                
                score = silhouette_score(features, labels)
                
                if score > best_score:
                    best_score = score
                    best_k = k
                    best_model = gmm
                    
                print(f"  å¿«é€ŸGMM K={k}: Score={score:.4f}")
                
            except Exception:
                continue
        
        if best_model:
            return {
                'model': best_model,
                'score': best_score,
                'n_clusters': best_k,
                'params': {'n_components': best_k}
            }
        return None
    
    def _retrain_best_model(self, sample_result):
        """åœ¨å®Œæ•´æ•¸æ“šä¸Šé‡æ–°è¨“ç·´æœ€ä½³æ¨¡å‹"""
        print(f"ğŸ”„ åœ¨å®Œæ•´æ•¸æ“šä¸Šé‡æ–°è¨“ç·´æœ€ä½³æ¨¡å‹...")
        
        method = self.best_method
        params = sample_result['params']
        
        try:
            if 'gpu' in method and self.use_gpu:
                model = cuKMeans(**params, random_state=42)
            elif 'minibatch' in method:
                model = MiniBatchKMeans(**params, random_state=42, batch_size=1000)
            elif 'kmeans' in method:
                model = KMeans(**params, random_state=42, n_init=3, max_iter=100)
            elif 'gmm' in method:
                model = GaussianMixture(**params, random_state=42, max_iter=50)
            elif method == 'hdbscan':
                model = hdbscan.HDBSCAN(**params)
            elif method == 'dbscan':
                model = DBSCAN(**params)
            elif method == 'agglomerative':
                model = AgglomerativeClustering(**params)
            elif method == 'spectral':
                model = SpectralClustering(**params, random_state=42)
            else:
                return sample_result
            
            labels = model.fit_predict(self.features_public)
            
            # è¨ˆç®—æ–°çš„åˆ†æ•¸
            if hasattr(labels, 'to_pandas'):
                labels = labels.to_pandas().values
            if hasattr(self.features_public, 'to_pandas'):
                features = self.features_public.to_pandas().values
            else:
                features = self.features_public
            
            # è™•ç†æœ‰å™ªè²é»çš„ç®—æ³•
            if method in ['hdbscan', 'dbscan']:
                valid_mask = labels != -1
                if sum(valid_mask) > 0:
                    valid_labels = labels[valid_mask]
                    valid_features = features[valid_mask]
                    if len(set(valid_labels)) >= 2:
                        score = silhouette_score(valid_features, valid_labels)
                    else:
                        score = sample_result['score']
                else:
                    score = sample_result['score']
            else:
                score = silhouette_score(features, labels)
            
            return {
                'model': model,
                'score': score,
                'n_clusters': sample_result['n_clusters'],
                'params': params
            }
            
        except Exception as e:
            print(f"âš ï¸  é‡æ–°è¨“ç·´å¤±æ•—ï¼Œä½¿ç”¨æ¡æ¨£çµæœ: {e}")
            return sample_result
    
    def predict_private_fast(self):
        """å¿«é€Ÿé æ¸¬ç§æœ‰æ•¸æ“š"""
        if self.best_model is None:
            print("âœ— æ²’æœ‰å¯ç”¨æ¨¡å‹")
            return None
        
        print(f"\nğŸ¯ ä½¿ç”¨ {self.best_method.upper()} å¿«é€Ÿé æ¸¬...")
        print(f"ğŸ“Š ç§æœ‰æ•¸æ“šå¤§å°: {len(self.private_data):,} æ¨£æœ¬")
        
        start_time = time.time()
        
        try:
            # å°æ–¼èƒ½ç›´æ¥é æ¸¬çš„æ¨¡å‹
            if any(name in self.best_method for name in ['kmeans', 'gmm']):
                if self.use_gpu and hasattr(self.best_model, 'predict'):
                    # GPUé æ¸¬
                    if hasattr(self.features_private, 'to_pandas'):
                        private_labels = self.best_model.predict(self.features_private)
                        private_labels = private_labels.to_pandas().values
                    else:
                        import cudf
                        private_df = cudf.DataFrame(self.features_private)
                        private_labels = self.best_model.predict(private_df).to_pandas().values
                else:
                    # CPUé æ¸¬
                    features = self.features_private.to_pandas().values if hasattr(self.features_private, 'to_pandas') else self.features_private
                    private_labels = self.best_model.predict(features)
            
            elif self.best_method in ['hdbscan', 'dbscan', 'agglomerative', 'spectral']:
                # é€™äº›ç®—æ³•éœ€è¦é‡æ–°fitæ•´å€‹æ•¸æ“šé›†
                print("âš ï¸  æ­¤ç®—æ³•éœ€è¦é‡æ–°fitæ•´å€‹æ•¸æ“šé›†ï¼Œå¯èƒ½éœ€è¦è¼ƒé•·æ™‚é–“...")
                
                # åˆä½µæ•¸æ“š
                combined_features = np.vstack([self.features_public, self.features_private])
                
                # ä½¿ç”¨ç›¸åŒåƒæ•¸é‡æ–°è¨“ç·´
                if self.best_method == 'hdbscan':
                    new_model = hdbscan.HDBSCAN(**self.best_params)
                elif self.best_method == 'dbscan':
                    new_model = DBSCAN(**self.best_params)
                elif self.best_method == 'agglomerative':
                    new_model = AgglomerativeClustering(**self.best_params)
                elif self.best_method == 'spectral':
                    new_model = SpectralClustering(**self.best_params, random_state=42)
                
                combined_labels = new_model.fit_predict(combined_features)
                
                # æå–ç§æœ‰æ•¸æ“šçš„æ¨™ç±¤
                private_labels = combined_labels[len(self.features_public):]
            
            else:
                print("âœ— ä¸æ”¯æ´çš„æ¨¡å‹é¡å‹")
                return None
            
            elapsed = time.time() - start_time
            print(f"âœ“ é æ¸¬å®Œæˆï¼è€—æ™‚: {elapsed:.1f}s")
            
            # é¡¯ç¤ºèšé¡åˆ†ä½ˆ
            unique_labels, counts = np.unique(private_labels, return_counts=True)
            print(f"\nğŸ“ˆ ç§æœ‰æ•¸æ“šèšé¡åˆ†ä½ˆ:")
            for label, count in zip(unique_labels, counts):
                percentage = count / len(private_labels) * 100
                if label == -1:
                    print(f"  å™ªè²é»: {count:,} æ¨£æœ¬ ({percentage:.1f}%)")
                else:
                    print(f"  èšé¡ {label}: {count:,} æ¨£æœ¬ ({percentage:.1f}%)")
            
            return private_labels
            
        except Exception as e:
            print(f"âœ— é æ¸¬å¤±æ•—: {e}")
            return None
    
    def save_results_fast(self, private_labels, filename="private_submission.csv"):
        """å¿«é€Ÿä¿å­˜çµæœ"""
        if private_labels is None:
            return None
        
        print(f"\nğŸ’¾ ä¿å­˜çµæœ: {filename}")
        
        submission = pd.DataFrame({
            'id': self.private_data['id'],
            'label': private_labels
        })
        
        # ç¢ºä¿æ•¸æ“šé¡å‹å„ªåŒ–
        submission['id'] = submission['id'].astype('int32')
        submission['label'] = submission['label'].astype('int32')
        
        # æ¨™ç±¤æ¨™æº–åŒ–ï¼ˆç¢ºä¿å¾0é–‹å§‹ä¸”é€£çºŒï¼Œè™•ç†å™ªè²é»-1ï¼‰
        unique_labels = sorted(set(private_labels))
        if -1 in unique_labels:
            # å¦‚æœæœ‰å™ªè²é»ï¼Œå°‡å…¶é‡æ–°æ¨™è¨˜ç‚ºæœ€å¤§æ¨™ç±¤+1
            max_label = max([l for l in unique_labels if l != -1])
            label_mapping = {-1: max_label + 1}
            for i, old_label in enumerate([l for l in unique_labels if l != -1]):
                label_mapping[old_label] = i
        else:
            label_mapping = {old_label: new_label for new_label, old_label in enumerate(unique_labels)}
        
        submission['label'] = submission['label'].map(label_mapping)
        
        # å¿«é€Ÿä¿å­˜
        submission.to_csv(filename, index=False)
        
        print(f"âœ“ æª”æ¡ˆå·²ä¿å­˜: {filename}")
        print(f"âœ“ è¨˜éŒ„æ•¸: {len(submission):,}")
        print(f"âœ“ èšé¡æ•¸: {len(set(submission['label']))}")
        
        return submission

def parse_arguments():
    """è§£æå‘½ä»¤è¡Œåƒæ•¸"""
    parser = argparse.ArgumentParser(description='Private Dataset èšé¡é æ¸¬')
    parser.add_argument('--algorithm', '-a', type=str, 
                       choices=['kmeans', 'minibatch_kmeans', 'gmm', 'hdbscan', 
                               'dbscan', 'agglomerative', 'spectral', 'gpu_kmeans'],
                       help='æŒ‡å®šè¦ä½¿ç”¨çš„èšé¡ç®—æ³•')
    parser.add_argument('--no-gpu', action='store_true', 
                       help='ç¦ç”¨GPUåŠ é€Ÿ')
    parser.add_argument('--no-sampling', action='store_true',
                       help='ç¦ç”¨æ¡æ¨£å„ªåŒ–')
    parser.add_argument('--output', '-o', type=str, default='private_submission.csv',
                       help='è¼¸å‡ºæª”æ¡ˆåç¨±')
    return parser.parse_args()

def main():
    """ä¸»å‡½æ•¸"""
    # è§£æå‘½ä»¤è¡Œåƒæ•¸
    args = parse_arguments()
    
    print("="*70)
    print("ğŸš€ å¤§æ•¸æ“šåˆ†ææœŸæœ«å°ˆæ¡ˆ - Private Dataset é«˜é€Ÿé æ¸¬")
    print("="*70)
    
    # åˆå§‹åŒ–é«˜é€Ÿé æ¸¬å™¨
    predictor = FastPrivatePredictor(
        use_gpu=not args.no_gpu,      # GPUåŠ é€Ÿ
        use_sampling=not args.no_sampling,  # æ¡æ¨£å„ªåŒ–
        n_jobs=-1,                    # ä½¿ç”¨æ‰€æœ‰CPUæ ¸å¿ƒ
        force_algorithm=args.algorithm # æŒ‡å®šç®—æ³•
    )
    
    total_start_time = time.time()
    
    # åŸ·è¡Œæµç¨‹
    steps = [
        ("è¼‰å…¥æ•¸æ“š", predictor.load_data),
        ("æ•¸æ“šé è™•ç†", predictor.preprocess_data_fast),
        ("æ¨¡å‹é¸æ“‡", predictor.find_best_model_fast),
    ]
    
    for step_name, step_func in steps:
        if not step_func():
            print(f"âœ— {step_name}å¤±æ•—ï¼Œç¨‹å¼çµæŸ")
            return None
    
    # é æ¸¬å’Œä¿å­˜
    private_labels = predictor.predict_private_fast()
    if private_labels is None:
        return None
    
    submission = predictor.save_results_fast(private_labels, args.output)
    
    # ç¸½çµ
    total_elapsed = time.time() - total_start_time
    print("\n" + "="*70)
    print("ğŸ‰ é«˜é€Ÿé æ¸¬å®Œæˆï¼")
    print("="*70)
    print(f"âš¡ ç¸½è€—æ™‚: {total_elapsed:.1f}s")
    print(f"ğŸ§  ä½¿ç”¨æ¨¡å‹: {predictor.best_method.upper()}")
    print(f"ğŸ“Š é æ¸¬æ¨£æœ¬æ•¸: {len(private_labels):,}")
    print(f"ğŸ¯ èšé¡æ•¸é‡: {len(set(private_labels))}")
    print(f"ğŸ“ è¼¸å‡ºæª”æ¡ˆ: {args.output}")
    
    if total_elapsed < 60:
        print(f"ğŸ† é æ¸¬é€Ÿåº¦: {len(private_labels)/total_elapsed:.0f} æ¨£æœ¬/ç§’")
    
    return submission

if __name__ == "__main__":
    submission = main() 